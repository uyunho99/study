{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "회귀 문제는 수치를 예측하는 지도 학습 문제이다. 회귀 문제를 푼다는 것은 주어진 데이터에 대해 관계를 나타내는 식을 가정하고, 해당 데이터에 가장 알맞은 식의 계수를 정해 나간다는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형 회귀와 최소제곱법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단순 선형 회귀(Univariate Linear Regression)는 회귀 문제 중에서 제일 기초가 되는 문제다. 단순 선형 회귀란 분석하고 싶은 두 변수의 관계를 가장 잘 설명하는 직선이 무엇인지를 구현하는 기법이다.\n",
    "\n",
    "여기서 분석하고 두 변수를 input(feature)과 output(target)으로 분류하고, 이 둘의 관계를 나타내는 함수(단순 선형 회귀에서는 직선이다)를 가설(Hypothesis)이라고 부른다. 단순 선형 회귀에서 가설은 다음과 같이 작성한다.\n",
    "\n",
    "$$\\mathrm{Hypothesis :} \\; h_{\\theta}(x)=\\theta_{0}+\\theta{1}x$$\n",
    "\n",
    "y절편이 $\\theta_{0}$고, 기울기가 $\\theta_{1}$인 우리가 흔히 아는 1차방정식이다. 하지만 모든 input과 output이 가설 함수 상에 존재하기란 불가능에 가깝다.\n",
    "\n",
    "여기서 $\\theta$들을 parameter라고 하는데, parameter를 조정하면서 데이터를 가장 잘 설명하는 가설 함수를 찾아야 한다. 이 때, 함수와 데이터 간 거리의 제곱의 합을 비용(cost)라고 하고, 비용 함수는 다음과 같이 작성한다.\n",
    "\n",
    "$$\\mathrm{Cost function :} \\; J(\\theta_{0}, \\theta_{1})=\\frac{1}{2m}\\sum\\limits_{i=1}^m(\\hat{y}^{i}-y^{i})^{2} = \\frac{1}{2m}\\sum\\limits_{i=1}^m(h_{\\theta}(x^{i})-y^{i})^{2}$$\n",
    "\n",
    "이것이 최소제곱법의 아이디어다. 최종적으로 우리는 비용이 작을수록 가설이 데이터를 잘 설명한다고 할 수 있기에, 비용함수를 최소화시키는 것이 선형 회귀에서 우리가 궁극적으로 추구해야 하는 목표라고 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
