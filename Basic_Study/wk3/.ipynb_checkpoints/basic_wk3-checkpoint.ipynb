{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 지도 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "머신러닝의 학습 방법은 크게 지도 학습(supervised learning)과 비지도 학습(unsupervised learning)으로 나눌 수 있다. \n",
    "- 지도 학습 : 라벨링이 된 데이터를 학습시키는 것을 의미\n",
    "- 비지도 학습 : 라벨링이 되지 않은 데이터를 학습 시키는 것을 의미\n",
    "\n",
    "(여기서 라벨링이란 학습시킬 데이터(train data)에 각각에 맞추고 싶은 정답(target data)이 표시된 것을 의미한다.)\n",
    "\n",
    "지도학습은 target data의 형태에 따라 분류(classification)와 회귀(regression)로 나눌 수 있다.\n",
    "- 분류 : target data가 categorical/discrete 형태인 경우, 데이터가 속한 그룹을 찾아내는 것을 의미\n",
    "- 회귀 : target data가 continuous 형태인 경우, feature variable과 target variable의 관계를 찾아내는 것을 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbblCkC%2FbtqEq8rEkUa%2FLM1DLkUgBku9ApbkTYX90K%2Fimg.png' width='600' height='300'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가설"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**단순 선형 회귀(Univariate Linear Regression)** 는 회귀 문제 중에서 제일 기초가 되는 문제다. 단순 선형 회귀란 분석하고 싶은 두 변수의 관계를 가장 잘 설명하는 직선이 무엇인지를 구현하는 기법이다.\n",
    "\n",
    "여기서 분석하고 두 변수를 input(feature)과 output(target)으로 분류하고, 이 둘의 관계를 나타내는 함수(단순 선형 회귀에서는 직선이다)를 **가설(Hypothesis)** 이라고 부른다. 단순 선형 회귀에서 가설은 다음과 같이 작성한다.\n",
    "\n",
    "$$\\mathrm{Hypothesis :} \\; h_{\\theta}(x)=\\theta_{0}+\\theta_{1}x$$\n",
    "\n",
    "- $\\theta_{1}$ : weight(가중치) of hypothesis\n",
    "- $\\theta_{0}$ : bias(편향) of hypothesis\n",
    "\n",
    "쉽게 말해, y절편이 $\\theta_{0}$고, 기울기가 $\\theta_{1}$인 우리가 흔히 아는 1차방정식이다. (가설에서 사용되는 $\\theta$들을 parameter라고 부르기도 한다.) 하지만 모든 x, y 조합(변수)이 가설 함수 상에 존재하기란 불가능에 가깝다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://wikidocs.net/images/page/53560/그림3.PNG' width='500' height='320'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비용 함수 : 평균 제곱 오차(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러므로 우리는 parameter $(\\theta_{0}, \\theta_{1})$를 조정하면서 데이터를 가장 잘 설명하는 가설 함수를 찾아야 한다.\n",
    "\n",
    "가설 함수와 개별 데이터 간 거리의 제곱의 합을 **비용(cost)** 이라고 한다. 실제 값과 예측 값의 거리이므로 오차라고 해석해도 좋다. 비용 함수는 다음과 같이 작성한다.\n",
    "\n",
    "$$\\mathrm{Cost \\; function :} \\; J(\\theta_{0}, \\theta_{1})=\\frac{1}{2m}\\sum\\limits_{i=1}^m(\\hat{y}^{i}-y^{i})^{2} = \\frac{1}{2m}\\sum\\limits_{i=1}^m(h_{\\theta}(x^{i})-y^{i})^{2}$$\n",
    "\n",
    "- $m$ : data size\n",
    "- $i$ : index number of individual data\n",
    "- $\\hat{y}^{i}$ : predicted value of individual data (by linear regression)\n",
    "\n",
    "위에서 규정한 가설함수를 통해 얻을 수 있는 예측치인 $\\hat{y}^{i}$와 원래 결과 값인 $y^{i}$의 차이의 제곱으로 규정한다. (여기서 $\\frac{1}{2}$로 나누는 이유는, 추후 경사하강법 단계에서 미분을 하므로 계산상의 편의를 위해서다.)\n",
    "\n",
    "이를 **평균 제곱 오차(MSE, Mean Squared Error)** 라고 부르기도 한다. 최종적으로 우리는 비용이 작을수록 가설이 데이터를 잘 설명한다고 할 수 있기에, 비용함수를 최소화시키는 것이 선형 회귀에서 우리가 궁극적으로 추구해야 하는 목표라고 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최적화 : 경사하강법(Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그렇다면 비용 함수를 어떻게 최소화시킬 것인가? 비용 함수를 최소화하기 위해 사용되는 알고리즘을 옵티마이저/최적화 알고리즘이라고 한다. 가장 기본적인 옵티마이저인 경사하강법에 대해 다뤄보겠다.\n",
    "\n",
    "기계는 1) 임의의 기울기 값을 정하고, 2) 최소값을 향해 기울기 값을 조정해가는데, 이 과정을 **경사하강법(Gradient Descent)** 이라고 한다. 이를 위해 비용 함수의 접선의 기울기가 제일 낮은 0인 곳을 향해, 지속적으로 업데이트 해가는 것이다. (위에서 평균 제곱 오차를 따르는 비용 함수의 경우에는, 2차 함수의 구조를 가지기에 볼록한 부분의 맨 아래 부분에서 cost가 가장 최소값을 가진다.)\n",
    "\n",
    "$$\\mathrm{(repeat \\; until \\; convergence)} \\; \\theta_{j} ≔ \\theta_{j} - \\alpha\\frac{\\partial}{\\partial\\theta_{j}}J(\\theta_{0},\\theta_{1})$$\n",
    "\n",
    "- ≔ :  assignment(update variable after calculation)\n",
    "- $\\alpha$ : learning rate\n",
    "- $\\frac{\\partial}{\\partial\\theta_{j}}J(\\theta_{0},\\theta_{1})$ : derivative term\n",
    "\n",
    "경사하강법의 식은 다음과 같이 작성할 수 있다. 비용 함수 J를 미분한 값에 학습률 $(\\alpha)$를 곱해 weight $(\\theta_{0})$와 bias $(\\theta_{1})$를 지속적으로 업데이트하는 것이다.(정확히 두 parameter 동시에 업데이트한다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://yganalyst.github.io/assets/images/ML/chap3/gd1.png' width='450' height='270'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법에서 중요한 파라미터는 스텝의 크기로, 학습률(learning rate) 하이퍼파라미터로 결정된다. 학습률이 너무 작으면 계산량이 많아져 시간이 오래 걸리고, 너무 크면 이전보다 비용함수가 커질 수가 있기에 적절한 값으로 설정해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://yganalyst.github.io/assets/images/ML/chap3/gd2.png' width='450' height='250'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법을 구현하려면 비용함수의 그래디언트를 계산해야 한다.(지속적으로 미분값을 계산해야 한다.) 이 과정에서 훈련 세트에서 얼만큼씩 계산하느냐에 따라 경사하강법의 구현방법을 선택할 수 있다.\n",
    "\n",
    "- **배치 경사하강법(Batch GD)** : 경사하강법 스텝에서 전체 훈련 세트 X에 대한 그래디언트를 계산\n",
    "\n",
    "    - 배치 크기가 크기 때문에 안정적으로 수렴하는 것은 좋으나, 너무 안정적이기 때문에 local optima 문제가 발생할 가능성이 있다.\n",
    "\n",
    "- **확률적 경사하강법(Stochastic GD)** : 경사하강법 스텝에서 1개의 샘플을 무작위로 선택하고 그 하나의 샘플에 대한 그래디언트를 계산\n",
    "\n",
    "    - 배치 경사하강법에 비해 속도가 빠르며, 전역 최솟값을 찾게 도와준다.\n",
    "    \n",
    "    - 여기서 일반적으로 한 반복에서 m(train data의 숫자)번 되풀이되는데, 이때 각 반복을 에포크(epoch)라고 한다.)\n",
    "    \n",
    "    - scikit-learn에서 `SGDRegressor`를 사용하면 된다.\n",
    "    \n",
    "- **미니배치 경사하강법(Mini Batch GD)** : 경사하강법 스텝에서 미니배치라 부르는 임의의 작은 샘플 세트에 대해 그래디언트를 계산\n",
    "\n",
    "    - 배치 경사하강법과 확률적 경사하강법의 개념을 혼합해서 사용한다고 이해해도 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 회귀에서 모델의 과대적합을 방지하기 위해 가중치를 제한함으로써 regularization를 가한다. 규제(regularization)는 비용함수에 가중치의 노름(norm)을 더한 함수를 목적 함수(Objective function)으로 설정해 가중치를 제한한다.\n",
    "\n",
    "- **릿지(Ridge) 회귀** : 릿지는 계수를 **제곱** 한 값을 기준으로 regularization를 적용\n",
    "    - L2 regularization를 적용한 모델이다.\n",
    "    - scikit-learn에서 `Ridge`를 사용하면 된다.\n",
    "\n",
    "$$\\mathrm{Cost \\; function :} \\; J(\\theta)=\\, \\mathrm{MSE}(\\theta)+\\lambda\\frac{1}{2}\\sum_{i=1}^{n}\\theta_{i}^{2}$$\n",
    "(여기서 MSE는 위의 비용함수에서 사용했던 평균제곱오차다.)\n",
    "\n",
    "- **라쏘(Lasso) 회귀** : 라쏘는 계수의 **절댓값** 을 기준으로 regularization를 적용\n",
    "    - L1 regularization를 적용한 모델이다.\n",
    "    - scikit-learn에서 `Lasso`를 사용하면 된다.\n",
    "\n",
    "$$\\mathrm{Cost \\; function :} \\; J(\\theta)=\\, \\mathrm{MSE}(\\theta)+\\lambda\\sum_{i=1}^{n}|\\theta_{i}|$$\n",
    "\n",
    "- $\\lambda$ : hyperparameter로 regularization의 강조를 조절함, alpha 값이 크면 규제 강도가 세지므로 계수 값이 더 줄고 보다 과소적합되도록 유도함\n",
    "- hyperparameter(하이퍼파라미터) : 모델에서 학습되는 parameter와 달리, 모델 밖에서 직접 규정해줘야 하는 parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**다중 선형 회귀** 는 단순 선형 회귀에서 input(feature) variable이 여러 개로 추가되었을 뿐이다.\n",
    " 중에서 제일 기초가 되는 문제다. 단순 선형 회귀란 분석하고 싶은 두 변수의 관계를 가장 잘 설명하는 직선이 무엇인지를 구현하는 기법이다.\n",
    "\n",
    "여기서 분석하고 두 변수를 input(feature)과 output(target)으로 분류하고, 이 둘의 관계를 나타내는 함수(단순 선형 회귀에서는 직선이다)를 **가설(Hypothesis)** 이라고 부른다. 단순 선형 회귀에서 가설은 다음과 같이 작성한다.\n",
    "\n",
    "$$\\mathrm{Hypothesis :} \\; h_{\\theta}(x)=\\theta_{0}+\\theta_{1}x$$\n",
    "\n",
    "- $\\theta_{1}$ : weight(가중치) of hypothesis\n",
    "- $\\theta_{0}$ : bias(편향) of hypothesis\n",
    "\n",
    "쉽게 말해, y절편이 $\\theta_{0}$고, 기울기가 $\\theta_{1}$인 우리가 흔히 아는 1차방정식이다. (가설에서 사용되는 $\\theta$들을 parameter라고 부르기도 한다.) 하지만 모든 x, y 조합(변수)이 가설 함수 상에 존재하기란 불가능에 가깝다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다항 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature variable과 target variable의 관계가 비선형 데이터이더라도 **다항 회귀**를 활용해 선형 모델로 활용할 수 있다.\n",
    "- scikit-learn에서 `PolynomialFeatures`를 사용하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 설명한 로직을 활용해 회귀 문제를 풀어보도록 하겠다. 아래의 예시는 머신러닝의 기초적인 패키지인 scikit-learn에 있는 캘리포니아 주택 가격 데이터를 활용해 서술했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "dataset = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset에서 DataFrame을 활용해 구조를 파악한 후, feature, target variable을 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  target  \n",
       "0    -122.23   4.526  \n",
       "1    -122.22   3.585  \n",
       "2    -122.24   3.521  \n",
       "3    -122.25   3.413  \n",
       "4    -122.25   3.422  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "df['target'] = dataset.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 여기서 AveRooms와 AveBedrms 항목 만을 가지고 변수를 파악하도록 하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>5.045455</td>\n",
       "      <td>1.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>6.114035</td>\n",
       "      <td>1.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>5.205543</td>\n",
       "      <td>1.120092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>5.329513</td>\n",
       "      <td>1.171920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>5.254717</td>\n",
       "      <td>1.162264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AveRooms  AveBedrms\n",
       "0      6.984127   1.023810\n",
       "1      6.238137   0.971880\n",
       "2      8.288136   1.073446\n",
       "3      5.817352   1.073059\n",
       "4      6.281853   1.081081\n",
       "...         ...        ...\n",
       "20635  5.045455   1.133333\n",
       "20636  6.114035   1.315789\n",
       "20637  5.205543   1.120092\n",
       "20638  5.329513   1.171920\n",
       "20639  5.254717   1.162264\n",
       "\n",
       "[20640 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,'AveRooms':'AveBedrms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = dataset.data[:,2:4]\n",
    "y_data = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#plt.scatter(x_data[:, 0], y_data, s=5, alpha=0.1)\n",
    "#plt.scatter(x_data[:, 1], y_data, s=5, alpha=0.1)\n",
    "#plt.axis([0,10,-1,6])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train data와 test data를 8:2 비율로 분할한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측 오차를 더 줄이기 위해 데이터 표준화를 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scale = StandardScaler()\n",
    "std_scale.fit(x_train)\n",
    "\n",
    "x_train_std = std_scale.transform(x_train)\n",
    "x_test_std = std_scale.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LinearRegression`을 활용해 선형회귀 모델을 만들고 학습시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.74182594 -0.66336237] 2.07023847383721\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train_std, y_train)\n",
    "print(lr.coef_, lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률적 경사하강법은 다음과 같이 `SGDRegressor`을 통해 구현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.72864076 -0.69430688] [2.04882458]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None)\n",
    "sgd.fit(x_train_std, y_train)\n",
    "print(sgd.coef_, sgd.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge 함수를 통해 L2 제약식을 적용한다. Ridge 함수에서 alpha 값은 양수여야 하며, 값이 클수록 강한 제약식을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.74158871 -0.66312775] 2.07023847383721\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=1)\n",
    "ridge.fit(x_train_std, y_train)\n",
    "print(ridge.coef_, ridge.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_y_predict = lr.predict(x_test_std)\n",
    "sgd_y_predict = sgd.predict(x_test_std)\n",
    "rid_y_predict = ridge.predict(x_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2차 다항 회귀는 다음과 같이 `PolynomialFeatures`를 통해 구현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = dataset.data[:,2:4]\n",
    "y_data = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00123915 -0.0038297 ] 2.068765542635659\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "x_data = poly.fit_transform(x_data)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train_std, y_train)\n",
    "print(lr.coef_, lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로지스틱 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가설"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**로지스틱 회귀(Logistic Regression)** 는 이름은 회귀이지만 분류(classification)을 다루는데 활용한다.\n",
    "\n",
    "로지스틱 회귀에서 가설은 다음과 같이 작성한다. 우리가 선형 회귀에서 사용했던 기존의 가설함수인 $h_{\\theta}(x) = \\theta^{T}x$의 형태는 결과값이 0과 1 사이의 범위를 만족하지 않는다. 따라서 이를 해결하기 위해 가설함수를 **시그모이드(Sigmoid) 함수** 에 대입해 문제를 푼다. (여기서 가설함수는 최대한 간단하게 작성해보았다.)\n",
    "\n",
    "$$ \\mathrm{sigmoid \\, function \\, :} \\;  \\, g(z) = 1/(1+e^{-z})$$\n",
    "\n",
    "$$ h_{\\theta}(x) = \\frac{1}{1+e^{-\\theta^{T}x}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(-5, 5, 0.1)\n",
    "\n",
    "plt.plot(x, 1/(1+np.exp(-x)))\n",
    "plt.grid(alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 시그모이드 함수는 모든 출력값이 0과 1 사이의 범위에 존재한다. 이제 우리는 로지스틱 회귀에서 분류할 때의 기준을 다음과 같은 규칙을 통해 결정하기로 한다.\n",
    "\n",
    "$$ \\begin{cases} h_{\\theta}(x) \\geq 0.5 \\rightarrow y=1 \\\\ h_{\\theta}(x) < 0.5 \\rightarrow y=0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비용 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 선형회귀에서 사용했던 비용함수를 그대로 적용하면 볼록함수가 아니기 때문에 다음과 같은 함수로 정의한다.\n",
    "\n",
    "$$Cost(h_{\\theta}(x), y) = \\begin{cases} -\\log(h_{\\theta}(x)) \\qquad \\; \\,  (y=1)\\\\ -\\log(1-h_{\\theta}(x)) \\quad (y=0)\\\\ \\end{cases}$$\n",
    "\n",
    "$y = 1$인 경우에, $h_{\\theta}(x)=1$이면 비용은 0이지만, $h_{\\theta}(x)=0$이면 비용은 무한대이다. (반대도 마찬가지다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다중 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로지스틱 회귀를 사용한 다중 분류의 매커니즘은 위의 방식과 동일하다. 다만 시그모이드 함수 대신 **소프트맥스(Softmax)** 를 사용한다. 소프트맥스 함수는 다음과 같이 표시할 수 있다.\n",
    "\n",
    "$$\\sigma_{i}=\\frac{e^{z_{i}}}{\\sum_{j=1}^{k} e^{z_{j}}}$$\n",
    "\n",
    "위의 시그모이드 함수에 z 자리에 기존 우리가 사용하던 가설 함수를 대입하듯이, 여기도 z에 각 분류 클래스를 설명하는 가설 함수($\\theta^{T}x$)를 대입하면 된다. 각자의 z값을 계산한 뒤, 소프트맥스 함수에 대입하면 k개의 분류 클래스 중 가장 확률이 높은 결과 클래스에 분류된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 설명한 로직을 활용해 로지스틱 회귀 문제를 풀어보도록 하겠다. 아래의 예시는 머신러닝의 기초적인 패키지인 scikit-learn에 있는 붗꽃 종류 데이터를 활용해 서술했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "dataset = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset에서 DataFrame을 활용해 구조를 파악한 후, feature, target variable을 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "df['target'] = dataset.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래에서 붗꽃의 종류가 3개인 것을 확인해볼 수 있다. 이진 분류를 진행하려면 `df = df[df.target != 2]` 다음과 같은 코드를 입력하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    50\n",
       "1    50\n",
       "2    50\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, -1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = dataset.data\n",
    "y_data = dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train data와 test data를 분할한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LogisticRegression`을 활용해 선형회귀 모델을 만들고 학습시킨다.\n",
    "\n",
    "- `penalty` : 기본적으로 Ridge 규제를 활용하므로 기본값은 'l2'임\n",
    "- `C` : 규제를 제어하는 매개변수, 작을수록 규제가 커짐, 기본값은 1임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', C=1.0, max_iter=1000)\n",
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로지스틱 회귀 방정식의 계수들은 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.41616425  0.87401056 -2.39740816 -0.9811188 ]\n",
      " [ 0.45706444 -0.30727281 -0.21964262 -0.69023726]\n",
      " [-0.04090019 -0.56673775  2.61705078  1.67135606]] [  9.59412535   2.2729443  -11.86706964]\n"
     ]
    }
   ],
   "source": [
    "print(lr.coef_, lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 계수들로 완성된 로지스틱 회귀 방정식에서 각 요소들을 대입한 결과를 알기 위해서는 `decision_function`을 활용하면 된다. 5번째 요소까지 대입하면 다음과 같은 값이 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.374  1.887  0.487]\n",
      " [ 5.845  2.743 -8.588]\n",
      " [-9.188  2.08   7.108]\n",
      " [-6.84   1.514  5.326]\n",
      " [ 6.693  3.046 -9.739]]\n"
     ]
    }
   ],
   "source": [
    "print(np.round(lr.decision_function(x_train[:5]), decimals=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 출력값을 기준으로 분류 클래스를 예측한 확률을 알기 위해서는 `predict_proba`를 활용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.011 0.793 0.196]\n",
      " [0.957 0.043 0.   ]\n",
      " [0.    0.007 0.993]\n",
      " [0.    0.022 0.978]\n",
      " [0.975 0.025 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.round(lr.predict_proba(x_train[:5]), decimals=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 결과를 통해, 로지스틱 회귀가 다음과 같이 예측했다는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(lr.predict(x_train[:5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d8dc7d3aa09af79666e556f106494fe4e31b040c6a4ab6a124d4dbda5928b8f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
