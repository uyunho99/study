{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Introduction to Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data enginner\n",
    "\n",
    "- Gather data from different sources\n",
    "\n",
    "- Optimized database for analyses\n",
    "\n",
    "- Removed corrupt data\n",
    "\n",
    "(Devolpe scalable data architecture > Streamline data acqusition > Set up processes to bring together data > Clean corrupt data > Well versed in cloud technology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Databases\n",
    "\n",
    "Processing > Scheduling > Using existed tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud services\n",
    "\n",
    "- Storage(AWS S3, Azure Blob Storage)\n",
    "\n",
    "- Computation(AWS EC2, Azure Virtual Machines)\n",
    "\n",
    "- Databases(AWS RDS, Azure SQL Database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data engineering toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL\n",
    "- Tables, Database schema, Relational databases\n",
    "\n",
    "#### NoSQL\n",
    "- Non-relational databases, Structured or unstructured, Key-value stores, Document DB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Databases schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the SELECT statement\n",
    "data = pd.read_sql(\"\"\"\n",
    "SELECT first_name, last_name FROM \"Customer\"\n",
    "ORDER BY last_name, first_name\n",
    "\"\"\", db_engine)\n",
    "\n",
    "# Show the first 3 rows of the DataFrame\n",
    "print(data.head(3))\n",
    "\n",
    "# Show the info of the DataFrame\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the SELECT statement\n",
    "data = pd.read_sql(\"\"\"\n",
    "SELECT * FROM \"Customer\"\n",
    "INNER JOIN \"Order\"\n",
    "ON \"Order\".\"customer_id\"=\"Customer\".\"id\"\n",
    "\"\"\", db_engine)\n",
    "\n",
    "# Show the id column of data\n",
    "print(data.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply a function over multiple cores\n",
    "from multiprocessing import Pool\n",
    "\n",
    "@print_timing\n",
    "def parallel_apply(apply_func, groups, nb_cores):\n",
    "    with Pool(nb_cores) as p:\n",
    "        results = p.map(apply_func, groups)\n",
    "    return pd.concat(results)\n",
    "\n",
    "# Parallel apply using 1 core\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), nb_cores = 1)\n",
    "\n",
    "# Parallel apply using 2 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), nb_cores = 2)\n",
    "\n",
    "# Parallel apply using 4 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), nb_cores = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Set the number of partitions\n",
    "athlete_events_dask = dd.from_pandas(athlete_events, npartitions=4)\n",
    "\n",
    "# Calculate the mean Age per Year\n",
    "print(athlete_events_dask.groupby('Year').Age.mean().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel computation frameworks\n",
    "\n",
    "#### Hadoop\n",
    "- Collection of open source packages for Big Data\n",
    "- HDFS is part of it\n",
    "- MapReduce is part of it\n",
    "\n",
    "#### PySpark\n",
    "- Python interface for the Spark framework\n",
    "- Uses DataFrame abstraction\n",
    "\n",
    "#### Hive\n",
    "- Built from the need to use structured queries for parallel processing\n",
    "- Initially used Hadoop MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the type of athlete_events_spark\n",
    "print(type(athlete_events_spark))\n",
    "\n",
    "# Print the schema of athlete_events_spark\n",
    "print(athlete_events_spark.printSchema())\n",
    "\n",
    "# Group by the Year, and find the mean Age\n",
    "print(athlete_events_spark.groupBy('Year').mean('Age'))\n",
    "\n",
    "# Group by the Year, and find the mean Age\n",
    "print(athlete_events_spark.groupBy('Year').mean('Age').show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow scheduling frameworks\n",
    "\n",
    "#### DAGs(Directed Acyclic Graph)\n",
    "- Set of nodes\n",
    "- Directed edges\n",
    "- No cycles\n",
    "\n",
    "#### Apache Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DAG object\n",
    "dag = DAG(dag_id=\"car_factory_simulation\",\n",
    "          default_args={\"owner\": \"airflow\",\"start_date\": airflow.utils.dates.days_ago(2)},\n",
    "          schedule_interval=\"0 * * * *\")\n",
    "\n",
    "# Task definitions\n",
    "assemble_frame = BashOperator(task_id=\"assemble_frame\", bash_command='echo \"Assembling frame\"', dag=dag)\n",
    "place_tires = BashOperator(task_id=\"place_tires\", bash_command='echo \"Placing tires\"', dag=dag)\n",
    "assemble_body = BashOperator(task_id=\"assemble_body\", bash_command='echo \"Assembling body\"', dag=dag)\n",
    "apply_paint = BashOperator(task_id=\"apply_paint\", bash_command='echo \"Applying paint\"', dag=dag)\n",
    "\n",
    "# Complete the downstream flow\n",
    "assemble_frame.set_downstream(place_tires)\n",
    "assemble_frame.set_downstream(assemble_body)\n",
    "assemble_body.set_downstream(apply_paint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract, Transform and Load (ETL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract\n",
    "- Send data in JSON format\n",
    "- API: application programming interface\n",
    "\n",
    "- Applications databases : Transactions, OLTP, Row-oriented\n",
    "- Analytical databases : OLAP, Column-oriented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Fetch the Hackernews post\n",
    "resp = requests.get(\"https://hacker-news.firebaseio.com/v0/item/16222426.json\")\n",
    "\n",
    "# Print the response parsed as JSON\n",
    "print(resp.json())\n",
    "\n",
    "# Assign the score of the test to post_score\n",
    "post_score = resp.json()[\"score\"]\n",
    "print(post_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract table to a pandas DataFrame\n",
    "def extract_table_to_pandas(tablename, db_engine):\n",
    "    query = \"SELECT * FROM {}\".format(tablename)\n",
    "    return pd.read_sql(query, db_engine)\n",
    "\n",
    "# Connect to the database using the connection URI\n",
    "connection_uri = \"postgresql://repl:password@localhost:5432/pagila\" \n",
    "db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "# Extract the film table into a pandas DataFrame\n",
    "extract_table_to_pandas(\"film\", db_engine)\n",
    "\n",
    "# Extract the customer table into a pandas DataFrame\n",
    "extract_table_to_pandas(\"customer\", db_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rental rate column as a string\n",
    "rental_rate_str = film_df.rental_rate.astype(str)\n",
    "\n",
    "# Split up and expand the column\n",
    "rental_rate_expanded = rental_rate_str.str.split('.', expand=True)\n",
    "\n",
    "# Assign the columns to film_df\n",
    "film_df = film_df.assign(\n",
    "    rental_rate_dollar=rental_rate_expanded[0],\n",
    "    rental_rate_cents=rental_rate_expanded[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use groupBy and mean to aggregate the column\n",
    "ratings_per_film_df = rating_df.groupBy('film_id').mean('rating')\n",
    "\n",
    "# Join the tables using the film_id column\n",
    "film_df_with_ratings = film_df.join(\n",
    "    ratings_per_film_df,\n",
    "    film_df.film_id==ratings_per_film_df.film_id\n",
    ")\n",
    "\n",
    "# Show the 5 first results\n",
    "print(film_df_with_ratings.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load\n",
    "\n",
    "#### Analytics\n",
    "- Aggregate queries\n",
    "- Online analytical processing(OLAP)\n",
    "- Column-oriented\n",
    "\n",
    "#### Applications\n",
    "- Lots of transactions\n",
    "- Online transaction processing(OLTP)\n",
    "- Row-oriented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the pandas DataFrame to parquet\n",
    "film_pdf.to_parquet(\"films_pdf.parquet\")\n",
    "\n",
    "# Write the PySpark DataFrame to parquet\n",
    "film_sdf.write.parquet('films_sdf.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish the connection URI\n",
    "connection_uri = \"postgresql://repl:password@localhost:5432/dwh\"\n",
    "db_engine_dwh = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "# Transformation step, join with recommendations data\n",
    "film_pdf_joined = film_pdf.join(recommendations)\n",
    "\n",
    "# Finish the .to_sql() call to write to store.film\n",
    "film_pdf_joined.to_sql(\"film\", db_engine_dwh, schema=\"store\", if_exists=\"replace\")\n",
    "\n",
    "# Run the query to fetch the data\n",
    "pd.read_sql(\"SELECT film_id, recommended_film_ids FROM store.film\", db_engine_dwh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ETL function\n",
    "def etl():\n",
    "    film_df = extract_film_to_pandas()\n",
    "    film_df = transform_rental_rate(film_df)\n",
    "    load_dataframe_to_film(film_df)\n",
    "\n",
    "# Define the ETL task using PythonOperator\n",
    "etl_task = PythonOperator(task_id='etl_film',\n",
    "                          python_callable=etl,\n",
    "                          dag=dag)\n",
    "\n",
    "# Set the upstream to wait_for_table and sample run etl()\n",
    "etl_task.set_upstream(wait_for_table)\n",
    "etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the connection URI\n",
    "connection_uri = \"postgresql://repl:password@localhost:5432/datacamp_application\"\n",
    "db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "# Get user with id 4387\n",
    "user1 = pd.read_sql(\"SELECT * FROM rating WHERE user_id=4387\", db_engine)\n",
    "\n",
    "# Get user with id 18163\n",
    "user2 = pd.read_sql(\"SELECT * FROM rating WHERE user_id=18163\", db_engine)\n",
    "\n",
    "# Get user with id 8770\n",
    "user3 = pd.read_sql(\"SELECT * FROM rating WHERE user_id=8770\", db_engine)\n",
    "\n",
    "# Use the helper function to compare the 3 users\n",
    "print_user_comparison(user1, user2, user3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the transformation function\n",
    "def transform_avg_rating(rating_data):\n",
    "    # Group by course_id and extract average rating per course\n",
    "    avg_rating = rating_data.groupby('course_id').rating.mean()\n",
    "    # Return sorted average ratings per course\n",
    "    sort_rating = avg_rating.sort_values(ascending=False).reset_index()\n",
    "    return sort_rating\n",
    "\n",
    "# Extract the rating data into a DataFrame    \n",
    "rating_data = extract_rating_data(db_engines)\n",
    "\n",
    "# Use transform_avg_rating on the extracted data and print results\n",
    "avg_rating_data = transform_avg_rating(rating_data)\n",
    "print(avg_rating_data) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the number of missing values per column\n",
    "print(course_data.isnull().sum())\n",
    "\n",
    "# The transformation should fill in the missing values\n",
    "def transform_fill_programming_language(course_data):\n",
    "    imputed = course_data.fillna({\"programming_language\": \"R\"})\n",
    "    return imputed\n",
    "\n",
    "transformed = transform_fill_programming_language(course_data)\n",
    "\n",
    "# Print out the number of missing values per column of transformed\n",
    "print(transformed.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the transformation function\n",
    "def transform_recommendations(avg_course_ratings, courses_to_recommend):\n",
    "    # Merge both DataFrames\n",
    "    merged = courses_to_recommend.merge(avg_course_ratings) \n",
    "    # Sort values by rating and group by user_id\n",
    "    grouped = merged.sort_values(\"rating\", ascending=False).groupby(\"user_id\")\n",
    "    # Produce the top 3 values and sort by user_id\n",
    "    recommendations = grouped.head(3).sort_values(\"user_id\").reset_index()\n",
    "    final_recommendations = recommendations[[\"user_id\", \"course_id\",\"rating\"]]\n",
    "    # Return final recommendations\n",
    "    return final_recommendations\n",
    "\n",
    "# Use the function with the predefined DataFrame objects\n",
    "recommendations = transform_recommendations(avg_course_ratings, courses_to_recommend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_uri = \"postgresql://repl:password@localhost:5432/dwh\"\n",
    "db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "def load_to_dwh(recommendations):\n",
    "    recommendations.to_sql(\"recommendations\", db_engine, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DAG so it runs on a daily basis\n",
    "dag = DAG(dag_id=\"recommendations\",\n",
    "          schedule_interval='0 0 * * *')\n",
    "\n",
    "# Make sure `etl()` is called in the operator. Pass the correct kwargs.\n",
    "task_recommendations = PythonOperator(\n",
    "    task_id=\"recommendations_task\",\n",
    "    python_callable=etl,\n",
    "    op_kwargs={\"db_engines\": db_engines},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendations_for_user(user_id, threshold=4.5):\n",
    "    # Join with the courses table\n",
    "    query = \"\"\"\n",
    "    SELECT title, rating FROM recommendations\n",
    "    INNER JOIN courses ON courses.course_id = recommendations.course_id\n",
    "    WHERE user_id=%(user_id)s AND rating>%(threshold)s\n",
    "    ORDER BY rating DESC\n",
    "    \"\"\"\n",
    "    # Add the threshold parameter\n",
    "    predictions_df = pd.read_sql(query, db_engine, params = {\"user_id\": user_id, \n",
    "                                                             \"threshold\": threshold})\n",
    "    return predictions_df.title.values\n",
    "\n",
    "# Try the function you created\n",
    "print(recommendations_for_user(12, 4.65))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
